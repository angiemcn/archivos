{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y5EkbZywD4z",
        "outputId": "8bc386b1-6aa8-415e-c7da-9a1f7a03e15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m973.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.3/906.3 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain_openai langchain-core langgraph langchain-community psycopg[binary,pool]==3.2.6 langgraph-checkpoint-postgres langchain-elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "with open(\"/content/api_key.txt\") as archivo:\n",
        "  apikey = archivo.read()\n",
        "os.environ[\"OPENAI_API_KEY\"] = apikey\n",
        "\n",
        "with open(\"/content/postgrest.txt\") as archivo:\n",
        "  uribd = archivo.read()"
      ],
      "metadata": {
        "id": "zSK7czbdwTns"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_elasticsearch import ElasticsearchStore\n",
        "from psycopg_pool import ConnectionPool\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "from langgraph.prebuilt import create_react_agent"
      ],
      "metadata": {
        "id": "HCsvoxG-wVIc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain.agents import Tool"
      ],
      "metadata": {
        "id": "6IBhuFZ9waCF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = SQLDatabase.from_uri(uribd)\n",
        "\n",
        "def get_schema() -> str:\n",
        "    return db.get_table_info()\n",
        "\n",
        "prompt_sql = ChatPromptTemplate.from_template(\"\"\"\n",
        "Dado el siguiente esquema de la base de datos y una pregunta, genera solamente una consulta SQL válida y completa.\n",
        "\n",
        "Si la pregunta menciona un código de pedido, genera una consulta que:\n",
        "Incluya el estado del pedido y la fecha de entrega.\n",
        "Traiga los productos relacionados a ese pedido.\n",
        "No uses comillas triples, bloques de código ni explicaciones.\n",
        "\n",
        "Esquema de la base de datos:\n",
        "{schema}\n",
        "\n",
        "Pregunta del usuario:\n",
        "{question}\n",
        "\n",
        "Consulta SQL (sin formato adicional):\n",
        "\"\"\")\n",
        "\n",
        "sql_chain = prompt_sql | ChatOpenAI(model=\"gpt-4.1-2025-04-14\").bind(stop=[\"\\nSQLResult\"]) | StrOutputParser()\n",
        "\n",
        "def generar_sql(schema: str, question: str) -> str:\n",
        "    return sql_chain.invoke({\"schema\": schema, \"question\": question})\n",
        "\n",
        "def run_query(query: str) -> str:\n",
        "    return db.run(query)\n",
        "\n",
        "prompt_respuesta = ChatPromptTemplate.from_template(\"\"\"\n",
        "Basado en el esquema, pregunta, SQL y resultado, responde en lenguaje natural.\n",
        "\n",
        "Esquema:\n",
        "{schema}\n",
        "\n",
        "Pregunta:\n",
        "{question}\n",
        "\n",
        "SQL:\n",
        "{query}\n",
        "\n",
        "Resultado:\n",
        "{response}\n",
        "\n",
        "Respuesta natural:\n",
        "\"\"\")\n",
        "\n",
        "sqlnatural_chain = prompt_respuesta | ChatOpenAI(model=\"gpt-4.1-2025-04-14\") | StrOutputParser()\n",
        "\n",
        "def generar_respuesta(schema: str, question: str, query: str, response: str) -> str:\n",
        "    return sqlnatural_chain.invoke({\n",
        "        \"schema\": schema,\n",
        "        \"question\": question,\n",
        "        \"query\": query,\n",
        "        \"response\": response\n",
        "    })"
      ],
      "metadata": {
        "id": "lNRXJTeIwgxs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consultar_pedido(pregunta: str) -> str:\n",
        "    schema = get_schema()\n",
        "    sql = generar_sql(schema, pregunta)\n",
        "    resultado = run_query(sql)\n",
        "    return generar_respuesta(schema, pregunta, sql, resultado)\n",
        "\n",
        "tool_consulta_pedido = Tool(\n",
        "    name=\"consulta_pedido\",\n",
        "    func=consultar_pedido,\n",
        "    description=\"Usa esta herramienta para obtener informacion detallada de un pedido.\"\n",
        ")"
      ],
      "metadata": {
        "id": "jq_rfnIPwj3U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_vector = ElasticsearchStore(\n",
        "    es_url=\"http://34.10.99.202:9200\",\n",
        "    es_user=\"elastic\",\n",
        "    es_password=\"qtPIwPpL7UamZQqjuqlm\",\n",
        "    index_name=\"lg-politicas\",\n",
        "    embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = db_vector.as_retriever()\n",
        "\n",
        "tool_rag =retriever.as_tool(\n",
        "        name=\"politicas\",\n",
        "        description=\"Consulta solo politicas de reclamos y reembolsos para determinar si procede o no el reembolso en un caso específico.\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvDuhBynxACD",
        "outputId": "482b255e-11dc-42f5-966b-7e4e2d43c214"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2314541577.py:10: LangChainBetaWarning: This API is in beta and may change in the future.\n",
            "  tool_rag =retriever.as_tool(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "Eres un asistente experto en seguimiento de pedidos y reclamos de una tienda de computadoras. Atiendes a los clientes de forma clara, empática y breve, como un amigo experto en tecnología.\n",
        "\n",
        "FLUJO DE ATENCIÓN:\n",
        "\n",
        "1. **Saludo e identificación**:\n",
        "   - Saluda cordialmente.\n",
        "   - Si el cliente no da un código de pedido, solicítalo antes de continuar.\n",
        "\n",
        "2. **Verificación del pedido**:\n",
        "   - Si el pedido está en curso, solo indícale que puede consultar el estado en la web.\n",
        "\n",
        "3. **Detección del problema**:\n",
        "   - Muestra al cliente los productos del pedido.\n",
        "   - Pregunta cuál(es) tienen problemas y qué tipo: **Faltante**, **Maltratado** o **Erróneo**.\n",
        "   - Solo continúas si el problema es uno de esos tres.\n",
        "\n",
        "4. **Revisión y resolución**:\n",
        "   - Verifica si el producto figura en el pedido.\n",
        "   - Usa la herramienta para consultar las políticas oficiales.\n",
        "   - Si tienes la fecha de entrega, calcula tú si el reclamo está dentro del plazo de 3 días hábiles (no preguntes al cliente).\n",
        "   - Si todo aplica, calcula el monto exacto del reembolso y comunícalo de forma directa:\n",
        "     - Menciona el porcentaje y el monto final.\n",
        "     - Indica que el reembolso será en **créditos de la app**, a menos que el cliente prefiera su tarjeta.\n",
        "   - **No repitas el texto de la política al cliente. Solo aplica la decisión con lenguaje natural.**\n",
        "\n",
        "5. **Cierre**:\n",
        "   - Agradece al cliente y pregunta si necesita algo más.\n",
        "\n",
        "REGLAS CLAVE:\n",
        "\n",
        "- Usa solo las herramientas disponibles.\n",
        "- No inventes reglas, productos ni estados.\n",
        "- No confirmes reembolsos si el producto no figura, el problema no es válido o el plazo expiró.\n",
        "- **No muestres ni expliques el texto exacto de las políticas al cliente. Aplica el criterio directamente.**\n",
        "- El reembolso por defecto se hace en **créditos de la app**. Solo si el cliente los rechaza, se ofrece devolución a tarjeta (5 a 10 días hábiles).\n",
        "\"\"\"),\n",
        "    (\"human\", \"{messages}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "ByMSX1eCxOeE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connection_kwargs = {\n",
        "    \"autocommit\": True,\n",
        "    \"prepare_threshold\": 0,\n",
        "}\n",
        "\n",
        "with ConnectionPool(\n",
        "    conninfo=uribd,\n",
        "    max_size=20,\n",
        "    kwargs=connection_kwargs,\n",
        ") as pool:\n",
        "    checkpointer = PostgresSaver(pool)\n",
        "    checkpointer.setup()"
      ],
      "metadata": {
        "id": "s5AGRbFMxYne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connection_kwargs = {\n",
        "    \"autocommit\": True,\n",
        "    \"prepare_threshold\": 0,\n",
        "}\n",
        "\n",
        "with ConnectionPool(\n",
        "    conninfo=uribd,\n",
        "    max_size=20,\n",
        "    kwargs=connection_kwargs,\n",
        ") as pool:\n",
        "    checkpointer = PostgresSaver(pool)\n",
        "\n",
        "    # Modelo\n",
        "    model = ChatOpenAI(model=\"gpt-4.1-2025-04-14\")\n",
        "\n",
        "    # Toolkit de herramientas\n",
        "    toolkit = [tool_rag, tool_consulta_pedido]\n",
        "\n",
        "    # Crear agente con prompt y memoria\n",
        "    agent_executor = create_react_agent(\n",
        "        model=model,\n",
        "        tools=toolkit,\n",
        "        checkpointer=checkpointer,\n",
        "        prompt=prompt\n",
        "    )\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": \"mc0035\"}}\n",
        "\n",
        "    # Ejemplo de ejecución\n",
        "    for step in agent_executor.stream(\n",
        "        {\"messages\": [HumanMessage(content=\"Hola\")]},\n",
        "        config=config,\n",
        "        stream_mode=\"values\",\n",
        "    ):\n",
        "        step[\"messages\"][-1].pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enz2Hwf0xgY1",
        "outputId": "8c2d8288-22d8-40c0-a28e-a027bded1bdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hola\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola! ¿En qué puedo ayudarte hoy? Si tienes un pedido y necesitas hacer seguimiento o tienes un reclamo, por favor dime tu código de pedido para poder revisarlo.\n"
          ]
        }
      ]
    }
  ]
}